{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8a5fc8b-9a29-43f4-a15c-b2424a73022d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Set Catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b21a68b-1cad-4158-b206-94adc7d72bf6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql. functions import col, trim, upper, regexp_replace, when, split, explode, array, lit, monotonically_increasing_id\n",
    "from delta.tables import DeltaTable\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "BASE_PATH = \"/Volumes/postnord/default/data/\"\n",
    "\n",
    "spark. sql(\"USE CATALOG postnord\")\n",
    "print(spark.catalog.currentCatalog())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1da4eae8-18c2-4ce5-b9be-eee66d251558",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "For Dynamic handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7544d700-fd7b-4d43-b3b6-fb50f41bb592",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "files = dbutils.fs.ls(BASE_PATH)\n",
    "print(f\"Found {len(files)} files\")\n",
    "\n",
    "basic_dates = set([\n",
    "    f.name.replace(\"Silver_item_basic_\", \"\").replace(\".csv\", \"\")\n",
    "    for f in files\n",
    "    if f.name.startswith(\"Silver_item_basic_\") and f.name.endswith(\".csv\")\n",
    "])\n",
    "\n",
    "scan_dates = set([\n",
    "    f.name.replace(\"Silver_item_scans_\", \"\").replace(\".csv\", \"\")\n",
    "    for f in files\n",
    "    if f.name.startswith(\"Silver_item_scans_\") and f.name. endswith(\".csv\")\n",
    "])\n",
    "\n",
    "complete_dates = sorted(basic_dates. intersection(scan_dates), reverse=True)\n",
    "\n",
    "if not complete_dates: \n",
    "    raise ValueError(\"No complete data sets found!\")\n",
    "\n",
    "latest_basic_date = complete_dates[0]\n",
    "latest_scan_date = complete_dates[0]\n",
    "\n",
    "expected_date = (datetime.now() - timedelta(days=1)).strftime(\"%Y%m%d\")\n",
    "if latest_basic_date < expected_date: \n",
    "    print(f\"Data delayed!  Latest:  {latest_basic_date}, Expected: {expected_date}\")\n",
    "else:\n",
    "    print(f\"Data is current\")\n",
    "\n",
    "print(f\"Latest Silver_item_basic date: {latest_basic_date}\")\n",
    "print(f\"Latest Silver_item_scans date: {latest_scan_date}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3087de51-a8b7-439b-8d5c-a8b0513b4aa8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_basic = (spark.read\n",
    "    . format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .load(f\"{BASE_PATH}Silver_item_basic_{latest_basic_date}.csv\"))\n",
    "\n",
    "display(df_basic. limit(5))\n",
    "df_basic.printSchema()\n",
    "\n",
    "# Merge (keys: packageid, transaction_ref, created_dt)\n",
    "if spark.catalog.tableExists(\"silver_item_basic\"):\n",
    "    DeltaTable.forName(spark, \"silver_item_basic\").alias(\"target\").merge(\n",
    "        df_basic.alias(\"source\"),\n",
    "        \"\"\"target. packageid <=> source.packageid \n",
    "           AND target.transaction_ref <=> source.transaction_ref \n",
    "           AND target. created_dt <=> source.created_dt\"\"\"\n",
    "    ).whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()\n",
    "    print(\"Merged into silver_item_basic\")\n",
    "else:\n",
    "    df_basic. write. format(\"delta\").saveAsTable(\"silver_item_basic\")\n",
    "    print(\"Created silver_item_basic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "871ec06a-6e65-45b1-ac81-5b2dd497210a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_scan = (spark.read\n",
    "    .format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .load(f\"{BASE_PATH}Silver_item_scans_{latest_scan_date}.csv\"))\n",
    "\n",
    "df_scan = (df_scan\n",
    "    .withColumnRenamed(\"created_dt\", \"scan_created_dt\")\n",
    "    .withColumnRenamed(\"Terminal\", \"scan_terminal\")\n",
    "    .withColumnRenamed(\"transaction_ref\", \"scan_transaction_ref\"))\n",
    "\n",
    "display(df_scan.limit(5))\n",
    "df_scan.printSchema()\n",
    "\n",
    "# Merge (keys: packageid, systemdato, scan_datetime, scan_type, location)\n",
    "if spark.catalog.tableExists(\"silver_item_scan\"):\n",
    "    DeltaTable.forName(spark, \"silver_item_scan\").alias(\"target\").merge(\n",
    "        df_scan.alias(\"source\"),\n",
    "        \"\"\"target.packageid <=> source.packageid \n",
    "           AND target. systemdato <=> source.systemdato\n",
    "           AND target.scan_datetime <=> source.scan_datetime\n",
    "           AND target.scan_type <=> source.scan_type\n",
    "           AND target.location <=> source.location\"\"\"\n",
    "    ).whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()\n",
    "    print(\"Merged into silver_item_scan\")\n",
    "else:\n",
    "    df_scan. write.format(\"delta\").saveAsTable(\"silver_item_scan\")\n",
    "    print(\"Created silver_item_scan\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a208b778-35db-4069-873b-70deda94e871",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "Ingest Data from Silver Tables (csvs) and EndTimeRules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bcc49eab-e441-49eb-b9eb-ce326c940339",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# df_basic = (spark.read\n",
    "#             .format(\"csv\")\n",
    "#             .option(\"header\", \"true\")\n",
    "#             .option(\"inferSchema\", \"true\")\n",
    "#             .load(f\"{base_path}Silver_item_basic_{latest_basic_date}.csv\"))\n",
    "# change head() method to limit()\n",
    "display(df_basic.limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd372b2e-39d1-4d87-ab60-8f7d24924445",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_basic.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c210becf-f9c3-4ae1-854a-2d8061ebb8af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# df_scan = (spark.read\n",
    "#            .format(\"csv\")\n",
    "#            .option(\"header\", \"true\")\n",
    "#            .option(\"inferSchema\", \"true\")\n",
    "#            .load(f\"{base_path}Silver_item_scans_{latest_basic_date}.csv\"))\n",
    "# change head() method to limit()\n",
    "display(df_scan.limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "686eabc9-c0f8-4a54-b577-ead607d6800f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_scan.printSchema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb194e26-eca3-41f5-a453-286d8d3f7fdd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# df_scan = df_scan.withColumnRenamed(\"created_dt\", \"scan_created_dt\")\n",
    "\n",
    "# df_scan = df_scan.withColumnRenamed(\"Terminal\", \"scan_terminal\")\n",
    "\n",
    "# df_scan = df_scan.withColumnRenamed(\"transaction_ref\", \"scan_transaction_ref\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82f1fe9f-a74b-4269-8a4e-2754224fcab3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# display(df_scan.printSchema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90911497-2ef8-42d8-b2bd-fec67e22a4fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#from pyspark.sql. functions import monotonically_increasing_id\n",
    "\n",
    "# Load Excel without header\n",
    "df_rules = spark.read. format(\"excel\").option(\"header\", \"false\").option(\"inferSchema\", \"true\").load(\"/Volumes/postnord/default/data/EndTimeRules.xlsx\")\n",
    "\n",
    "# Get the first row values (these are actual column names)\n",
    "header = df_rules.first()\n",
    "\n",
    "#print(header)\n",
    "\n",
    "# Add row index\n",
    "df_rules = df_rules.withColumn(\"row_id\", monotonically_increasing_id())\n",
    "\n",
    "# Remove first row\n",
    "df_rules = df_rules. filter(df_rules[\"row_id\"] > 0).drop(\"row_id\")\n",
    "\n",
    "# Rename columns using header values\n",
    "for i, col_name in enumerate(header):\n",
    "    df_rules = df_rules. withColumnRenamed(f\"_c{i}\", str(col_name))\n",
    "\n",
    "# The limit() method returns a spark dataframe\n",
    "display(df_rules.limit(5)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf7e78eb-1dd7-42d6-9729-7790e3750a14",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_rules.printSchema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85917a6b-d42f-400e-ba46-457d7bae8add",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_rules = df_rules.withColumnRenamed(\"location \", \"location\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8153a19-67ab-4270-9838-2e0a8f6b6ed0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_rules.printSchema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0387985-6d8d-430b-8aa5-479468f892d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql. functions import col, trim, upper, regexp_replace, when, split, explode, array, lit\n",
    "\n",
    "# ============================================\n",
    "# Clean and normalize df_rules \n",
    "# ============================================\n",
    "\n",
    "# Step 1: Clean scan_type - trim, uppercase, remove extra spaces\n",
    "df_rules = df_rules.withColumn(\n",
    "    \"scan_type_cleaned\",\n",
    "    upper(trim(regexp_replace(col(\"scan_type\"), r\"\\s+\", \" \")))\n",
    ")\n",
    "\n",
    "# Step 2: Handle \"OR\" logic - split scan_type into array\n",
    "# e.g., \"I OR L\" becomes [\"I\", \"L\"]\n",
    "df_rules = df_rules.withColumn(\n",
    "    \"scan_type_list\",\n",
    "    split(regexp_replace(col(\"scan_type_cleaned\"), r\"\\s*OR\\s*\", \",\"), \",\")\n",
    ")\n",
    "\n",
    "# Step 3: Explode to create one row per scan_type\n",
    "df_rules_exploded = df_rules.withColumn(\n",
    "    \"scan_type_normalized\",\n",
    "    explode(col(\"scan_type_list\"))\n",
    ").withColumn(\n",
    "    \"scan_type_normalized\",\n",
    "    trim(col(\"scan_type_normalized\"))\n",
    ")\n",
    "\n",
    "# Step 4: Clean reason_code and location\n",
    "df_rules_exploded = df_rules_exploded.withColumn(\n",
    "    \"reason_code\",\n",
    "    trim(col(\"reason_code\"))\n",
    ").withColumn(\n",
    "    \"location\",\n",
    "    when(col(\"location\").isNull(), None).otherwise(upper(trim(col(\"location\"))))\n",
    ")\n",
    "\n",
    "# ============================================\n",
    "# Step 5: Get all unique products from df_basic dynamically\n",
    "# ============================================\n",
    "product_rows = df_basic.select(\"product\").distinct().collect()\n",
    "all_product_codes = [row. product for row in product_rows]\n",
    "print(f\"Product codes found in data: {all_product_codes}\")\n",
    "\n",
    "# ============================================\n",
    "# Step 6: Create explicit product code arrays\n",
    "# ============================================\n",
    "# Build arrays with actual product codes\n",
    "all_products_array = array(*[lit(code) for code in all_product_codes])\n",
    "products_except_Z_array = array(*[lit(code) for code in all_product_codes if code != \"Z\"])\n",
    "\n",
    "df_rules_exploded = df_rules_exploded.withColumn(\n",
    "    \"product_codes\",\n",
    "    when(\n",
    "        col(\"product\").contains(\"also Z\"),\n",
    "        all_products_array  # Creates array like [\"W\", \"P\", \"Z\", \"G\"]\n",
    "    ).when(\n",
    "        col(\"product\").contains(\"except Z\"),\n",
    "        products_except_Z_array  # Creates array like [\"W\", \"P\", \"G\"]\n",
    "    ).otherwise(\n",
    "        all_products_array  # Default:  all product codes\n",
    "    )\n",
    ")\n",
    "\n",
    "# Step 7: Explode to create one row per product code\n",
    "df_rules_final = df_rules_exploded. withColumn(\n",
    "    \"product\",\n",
    "    explode(col(\"product_codes\"))\n",
    ")\n",
    "\n",
    "# Step 8: Select final columns for the rules table\n",
    "df_rules_final = df_rules_final.select(\n",
    "    col(\"reason_code\"),\n",
    "    col(\"scan_type_normalized\").alias(\"scan_type\"),\n",
    "    col(\"location\"),\n",
    "    col(\"product\"),\n",
    "    col(\"is_end_event\")\n",
    ").distinct()\n",
    "\n",
    "# ============================================\n",
    "# Display results to verify\n",
    "# ============================================\n",
    "print(\"=== Cleaned and Expanded Rules ===\")\n",
    "display(df_rules_final.orderBy(\"reason_code\", \"scan_type\", \"product\"))\n",
    "\n",
    "print(f\"\\nExpanded rule count: {df_rules_final. count()}\")\n",
    "\n",
    "# Show sample of expanded rules\n",
    "print(\"\\n=== Sample:  Rules with product 'Z' ===\")\n",
    "display(df_rules_final.filter(col(\"product\") == \"Z\").limit(5))\n",
    "\n",
    "print(\"\\n=== Sample:  Rules with product 'W' ===\")\n",
    "display(df_rules_final.filter(col(\"product\") == \"W\").limit(5))\n",
    "\n",
    "# Replace df_rules with the cleaned version\n",
    "df_rules = df_rules_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aca19e06-aa25-473a-ac14-d31ad9caf808",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_rules.printSchema())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93154a15-3ba0-44d3-bdc0-45cdeed5ece9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "creating delta tables so next notebook can pickup where we left off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89f03769-f091-49b7-8583-63b56f7a94d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "# Save df_basic as a Delta table\n",
    "df_basic.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"silver_item_basic\")\n",
    "\n",
    "# Save df_scan as a Delta table\n",
    "df_scan.write.format(\"delta\").mode(\"overwrite\").option(\n",
    "    \"overwriteSchema\", \"true\"\n",
    ").saveAsTable(\"silver_item_scan\")\n",
    "\n",
    "# Save df_rules as a Delta table\n",
    "df_rules.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"end_time_rules\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b422ec2-5623-486b-b998-751c1dd83e5c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#display(spark.sql(\"DESCRIBE HISTORY silver_item_scan\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54bbed0b-454e-404a-9c0e-178ce7715a62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Query data as it was 7 days ago\n",
    "#df_old = spark.read.format(\"delta\").option(\"timestampAsOf\", \"2025-12-28\").table(\"silver_item_scan\")\n",
    "\n",
    "# Or by version number\n",
    "#df_v2 = spark.read.format(\"delta\").option(\"versionAsOf\", 2).table(\"silver_item_scan\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "load_data",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
